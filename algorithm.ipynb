{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import swifter\n",
    "import shutil\n",
    "import os\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "nlp.max_length = 2000000\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm\n",
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phrase:\n",
    "    def __init__(self, type, dep, tokens=[], sub_phrases=[]):\n",
    "        self.tokens = tokens\n",
    "        self.type=type\n",
    "        self.sub_phrases = sub_phrases\n",
    "        self.parsing = None\n",
    "        self.pronouns = None\n",
    "        self.index = 0\n",
    "        self.dep = dep\n",
    "        self.word_category = []\n",
    "\n",
    "    def __str__(self):\n",
    "        sub_ph_str = \"\\n  SUBS:\"\n",
    "        for sub in self.sub_phrases:\n",
    "            sub_ph_str = sub_ph_str + \"\\n    \" + str(sub)\n",
    "        if len(self.sub_phrases) == 0:\n",
    "            sub_ph_str = \"\"\n",
    "        return \"Phrase(\" + str(self.type) + \" , \" + str(self.dep) + \" , \" + \" \".join([token.text for token in self.tokens]) + \")\" + sub_ph_str\n",
    "\n",
    "    def do_parsing(self):\n",
    "        for sub in self.sub_phrases:\n",
    "            sub.do_parsing()\n",
    "        \n",
    "        all_categories = []\n",
    "        for token in self.tokens:\n",
    "            if token.pos_ != \"PRON\":\n",
    "                cats = self.get_word_category(token.text)\n",
    "                if cats is not None:\n",
    "                    self.word_category.append(cats)\n",
    "                    all_categories.extend(cats)\n",
    "\n",
    "        has_person = False\n",
    "        has_group = False\n",
    "        if all_categories is not None:\n",
    "            if \"noun.person\" in all_categories:\n",
    "                has_person = True\n",
    "            if \"noun.group\" in all_categories:\n",
    "                has_group = True\n",
    "\n",
    "        # DO Parsing\n",
    "        self.parsing = {\n",
    "            \"contains_name\": self.contains_name(),\n",
    "            \"contains_orga\": self.contains_orga(),\n",
    "            \"contains_person\": has_person,\n",
    "            \"contain_group\": has_group,#self.contain_group(),\n",
    "            \"contain_collective_noun\": self.contain_collective_noun(),\n",
    "            \"contain_generic_noun\": self.contain_generic_noun(),\n",
    "            \"is_plural\": self.is_plural(),\n",
    "            \"plural_is_neutral\": self.plural_is_neutral(),\n",
    "            \"categories\": list(set(all_categories))\n",
    "        }\n",
    "\n",
    "        self.parse_pronouns()\n",
    "        self.order_tokens()\n",
    "\n",
    "    def order_tokens(self):\n",
    "        self.tokens = sorted(self.tokens, key=lambda x: x.i)\n",
    "        self.index = self.tokens[0].i\n",
    "\n",
    "    # \n",
    "    def parse_pronouns(self):\n",
    "        pronouns = []\n",
    "        for token in self.tokens:\n",
    "            if token.pos_ == \"PRON\":\n",
    "\n",
    "                case = None\n",
    "                if len(token.morph.get(\"Case\")) > 0:\n",
    "                    case = token.morph.get(\"Case\")[0]\n",
    "\n",
    "                plural = False\n",
    "                if len(token.morph.get(\"Number\")) > 0:\n",
    "                    plural = token.morph.get(\"Number\")[0] == \"Plur\"\n",
    "\n",
    "                gender = None\n",
    "                if len(token.morph.get(\"Gender\")) > 0:\n",
    "                    gender = token.morph.get(\"Gender\")[0]\n",
    "\n",
    "                poss = False\n",
    "                if len(token.morph.get(\"Poss\")) > 0:\n",
    "                    poss = token.morph.get(\"Poss\")[0] == \"Yes\"\n",
    "\n",
    "                reflex = False\n",
    "                if len(token.morph.get(\"Reflex\")) > 0:\n",
    "                    reflex = token.morph.get(\"Reflex\")[0] == \"Yes\"\n",
    "\n",
    "                person = None\n",
    "                if len(token.morph.get(\"Person\")) > 0:\n",
    "                    person = token.morph.get(\"Person\")[0]\n",
    "\n",
    "                # Pro Pronomen den Bezug klÃ¤ren. Z.B. Obj pronomen -> Sub. / Prev Sub \n",
    "                # Wenn Possesiv Pronomen subject ist kann es sich auf object oder Subject des vorherigen Clause beziehen. \n",
    "                \n",
    "                relates_to = []\n",
    "                if reflex == True or poss == True:\n",
    "                    relates_to.append(\"dsubj\")\n",
    "\n",
    "                if poss == True:\n",
    "                    relates_to.append(\"psubj\")\n",
    "                \n",
    "                pronouns.append(\n",
    "                {\n",
    "                    \"token\": token,\n",
    "                    # \"text\": token.text, \n",
    "                    # \"lemma\": token.lemma_,\n",
    "                    \"person\": person,\n",
    "                    \"case\": case,\n",
    "                    \"plural\": plural,\n",
    "                    # \"gender\": gender,\n",
    "                    # \"poss\": poss,\n",
    "                    # \"reflex\": reflex,\n",
    "                    \"type\": \"refl\" if reflex == True else (\"poss\" if poss == True else \"pers\"), \n",
    "                    \"relates_to\": relates_to\n",
    "                })\n",
    "        self.pronouns = pronouns\n",
    "                \n",
    "    def get_all_pronouns(self):\n",
    "        pronouns = []\n",
    "        pronouns.append(self.pronouns)\n",
    "        for sub in self.sub_phrases:\n",
    "            pronouns.append(sub.pronouns)\n",
    "        \n",
    "        return pronouns\n",
    "\n",
    "    # Different Parsing functions\n",
    "    def contains_name(self):\n",
    "        for token in self.tokens:\n",
    "            if \"PERSON\" in token.ent_type_:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def contains_orga(self):\n",
    "        for token in self.tokens:\n",
    "            if \"ORG\" in token.ent_type_:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def get_word_category(self, word):\n",
    "\n",
    "        synsets = wn.synsets(word)\n",
    "        if not synsets:\n",
    "            return None\n",
    "        categories = set()\n",
    "        for synset in synsets:\n",
    "            categories.add(synset.lexname())\n",
    "        \n",
    "        return categories\n",
    "\n",
    "    def contain_group(self):\n",
    "        for token in self.tokens:\n",
    "            if \"conj\" in token.dep_:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def contain_collective_noun(self):\n",
    "        collective_nouns = [\"people\",\"army\",\"audience\",\"band\",\"board\",\"cast\",\"choir\",\"class\",\"club\",\"coalition\",\"committee\",\"community\",\"company\",\"congregation\",\"corporation\",\"council\",\"crew\",\"crowd\",\"family\",\"firm\",\"gang\",\"group\",\"jury\",\"majority\",\"minority\",\"mob\",\"orchestra\",\"panel\",\"parliament\",\"party\",\"public\",\"school\",\"staff\",\"team\",\"troupe\"]\n",
    "        for token in self.tokens:\n",
    "            if token.lemma_ in collective_nouns:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def contain_generic_noun(self):\n",
    "        neutral_nouns = 'Teenager|Challenger|Entertainer|Leader|Acquaintance|Catcher|Representative|Guide|Skier|Champion|Lifeguard|Biker|Consultant|Driver|Patient|Jogger|Author|Seafarer|Protector|Researcher|Comedian|Navigator|Sailor|Sibling|Master|Elderly|Nurse|Authority|Journalist|Lobbyist|Spectator|Activist|Pitcher|Competitor|Expert|Director|Child|Enthusiast|Counselor|Orator|Novelist|Peacemaker|Snowboarder|Manager|Officer|Trainee|Keeper|Helper|Intern|Negotiator|Classmate|Shortstop|Novice|Student|Supervisor|Doctor|Mariner|Captain|Coach|Artist|Official|Chief|Controller|Learner|Skateboarder|Singer|Dancer|Judge|Scholar|Spouse|Advocate|Trekker|Instructor|Jumper|Backpacker|Fielder|inistrator|Nomad|Boss|Advisor|Acrobat|Stranger|Politician|Batter|Pilot|Pupil|Cyclist|Runner|Apprentice|Correspondent|Fan|Academic|Rival|Umpire|Climber|Diplomat|Moderator|Demonstrator|Colleague|Volunteer|Outfielder|Writer|Ambassador|Diver|Executive|Specialist|Traveler|Speaker|Juggler|Watcher|Mediator|Someone|Mentor|Adventurer|Guardian|Wanderer|Marathoner|Athlete|Rescuer|Spokesperson|Resident|Steward|Professional|Roommate|Scientist|Linesman|Technician|Teacher|Observer|Amateur|Supporter|Campaigner|Citizen|Narrator|Analyst|Therapist|Player|Caregiver|Coordinator|Delegate|Explorer|Thrower|Mime|Adult|Neighbor|Tourist|Racer|Delivery person|Organizer|Baseman|Envoy|Magician|Hobbyist|Expeditionist|Partner|Storyteller|Defender|Clown|Educator|Motorcyclist|Messenger|Swimmer|Hurdler|Relative|Mountaineer|Friend|Reporter|Infielder|Follower|Participant|Actress|Fisherman|Caretaker|Reviewer|Sprinter|Playwright|Host|Executi|Protester|Broadcaster|Surfer|Tutor|Opponent|Custodian|Musician|Referee|Critic|Hiker|Presenter|Courier|Arbitrator|Parent|Administrator|Poet|Actor|Hitter|Trainer|Performer|Debater|Illusionist|Companion|Cousin'\n",
    "        generic_nouns = neutral_nouns.lower().split(\"|\")\n",
    "        for token in self.tokens:\n",
    "            if token.lemma_ in generic_nouns:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def is_plural(self):\n",
    "        for token in self.tokens:\n",
    "            if \"Plur\" in token.morph.get(\"Number\"):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def plural_is_neutral(self):\n",
    "        gendered_words = pd.read_json(\"./gendered_words.json\")\n",
    "        gendered_words['word'] = gendered_words['word'].str.replace(\"_\", \" \")\n",
    "\n",
    "        for token in self.tokens:\n",
    "            if \"Plur\" in token.morph.get(\"Number\"):\n",
    "                if len(gendered_words.loc[gendered_words['word'].str.strip().str.lower() == token.lemma_.strip().lower()]) > 0:\n",
    "                    if gendered_words.loc[gendered_words['word'].str.strip().str.lower() == token.lemma_.strip().lower()]['gender'].to_list()[0] in [\"m\", \"f\"]:\n",
    "                        return False\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def get_phrase_classification(self):\n",
    "        classification = None\n",
    "\n",
    "        indefinite_pronouns = [\"all\", \"another\", \"any\", \"anybody\", \"anyone\", \"anything\", \"each\", \"everybody\", \"everyone\", \"everything\", \"few\", \"many\", \"nobody\", \"none\", \"one\", \"several\", \"some\", \"somebody\", \"someone\", \"something\"]\n",
    "        indefinite_sing_pronouns = [\"another\", \"any\", \"anybody\", \"anyone\", \"anything\", \"each\", \"everybody\", \"everyone\", \"everything\", \"nobody\", \"one\", \"somebody\", \"someone\", \"something\"]\n",
    "        indefinite_plur_pronouns = [\"few\", \"many\", \"several\"]\n",
    "        indefinite_context = [\"all\", \"some\", \"none\"]\n",
    "        # Sonderfall all, some and none ?!\n",
    "        \n",
    "        if self.parsing['contains_name'] == True and self.parsing['contain_group'] == False:\n",
    "            classification = \"SINGULAR\"\n",
    "        elif self.parsing['contains_person'] == True and self.parsing['contain_group'] == False and self.parsing['is_plural'] != True:\n",
    "            classification = \"GENERIC\"\n",
    "        elif (self.parsing['contain_group'] == True or self.parsing['contain_collective_noun'] == True or self.parsing['is_plural'] == True) and len(self.pronouns) == 0:\n",
    "            classification = \"PLURAL\"\n",
    "\n",
    "\n",
    "        if self.pronouns is not None:\n",
    "            for pronoun in self.pronouns:\n",
    "                if pronoun['token'].text.strip().lower() in indefinite_sing_pronouns:\n",
    "                    return \"GENERIC\"\n",
    "                elif pronoun['token'].text.strip().lower() in indefinite_plur_pronouns:\n",
    "                    return \"PLURAL\"\n",
    "                \n",
    "        return classification\n",
    "\n",
    "class Clause:\n",
    "    def __init__(self, type = \"\", phrases=[], sub_clauses=\"\"):\n",
    "        self.type = type\n",
    "        self.subj = []\n",
    "        self.obj = []\n",
    "        self.sub_clauses = sub_clauses\n",
    "        self.phrases = phrases\n",
    "        self.index = 0\n",
    "\n",
    "    def __str__(self):\n",
    "        if len(self.phrases) > 0:\n",
    "            phrases_str = \"\\n   Phrases:\"\n",
    "            for phrase in self.phrases:\n",
    "                phrases_str = phrases_str + \"\\n     \" + str(phrase)\n",
    "        else:\n",
    "            phrases_str = \"\"\n",
    "\n",
    "        return \"Clause(\" + str(self.type) + \"\\n SUBJ: \" + str(self.subj) + \"\\n OBJ: \" + str(self.obj) # + phrases_str\n",
    "\n",
    "    def append_phrases(self, phrases):\n",
    "        for phrase in phrases:\n",
    "            if phrase.type == \"subj\":\n",
    "                self.subj.append(phrase)\n",
    "            if phrase.type == \"obj\":\n",
    "                self.obj.append(phrase)\n",
    "\n",
    "    def do_parsing(self):\n",
    "        for sub in self.subj:\n",
    "            sub.do_parsing()\n",
    "        for sub in self.obj:\n",
    "            sub.do_parsing()\n",
    "        # DO Parsing\n",
    "        self.subj = sorted(self.subj, key=lambda x: x.index)\n",
    "        self.obj = sorted(self.obj, key=lambda x: x.index)\n",
    "\n",
    "        phrases = self.subj.copy()\n",
    "        phrases.extend(self.obj)\n",
    "        \n",
    "        if len(phrases) == 0:\n",
    "            self.index = 0\n",
    "        else:\n",
    "            self.index = sorted(phrases, key=lambda x: x.index)[0].index\n",
    "\n",
    "    def pronom_relation(self, prev_clauses=[]):\n",
    "        # for sub in self.subj:\n",
    "        resolutions = []\n",
    "        prev_clauses = prev_clauses.copy()\n",
    "        \n",
    "        subj_clause = Clause(type=\"SUBJ\")\n",
    "        subj_clause.subj = self.subj\n",
    "        prev_clauses.append(subj_clause)\n",
    "\n",
    "        prev_clauses.reverse()\n",
    "\n",
    "        parsed_subj = False\n",
    "\n",
    "        obj_index = 0     \n",
    "        for sub in self.obj:\n",
    "            pronoms = sub.get_all_pronouns()\n",
    "            subindex = 0\n",
    "            for pronoms_of_sub in pronoms:\n",
    "                for pronom in pronoms_of_sub:\n",
    "                    resolution, parsed_subj = self.resolve_single_pronom(\"OBJ\", pronom, subindex, sub, prev_clauses, parsed_subj, obj_index)\n",
    "                    if resolution is not None:\n",
    "                            resolutions.append(resolution)\n",
    "\n",
    "                subindex = subindex + 1\n",
    "            obj_index = obj_index + 1\n",
    "        \n",
    "        subj_index = 0     \n",
    "        # Parse Subject\n",
    "        if parsed_subj == False:\n",
    "            for sub in self.subj:\n",
    "                pronoms = sub.get_all_pronouns()\n",
    "                subindex = 0\n",
    "                for pronoms_of_sub in pronoms:\n",
    "                    for pronom in pronoms_of_sub:\n",
    "                        resolution, parsed_subj = self.resolve_single_pronom(\"SUBJ\", pronom, subindex, sub, prev_clauses, parsed_subj, subj_index)\n",
    "                        if resolution is not None:\n",
    "                            resolutions.append(resolution)\n",
    "\n",
    "                    subindex = subindex + 1\n",
    "                subj_index = subj_index + 1\n",
    "        \n",
    "        return resolutions\n",
    "    \n",
    "    def resolve_single_pronom(self, type, pronom, subindex, sub, prev_clauses, parsed_subj, index):\n",
    "        \n",
    "        relates_to_prev = False\n",
    "        resolution = PronomResolution(pronom)\n",
    "\n",
    "        # Possesiv Check \n",
    "        if pronom['type'] == \"poss\":\n",
    "            if subindex > 1:\n",
    "                for neighbour in sub.sub_phrases[0:(subindex-1)]:\n",
    "                    if neighbour.get_phrase_classification() is not None:\n",
    "                        resolution.relates_to = neighbour\n",
    "                        resolution.classification = neighbour.get_phrase_classification()\n",
    "                        return (resolution, parsed_subj)\n",
    "                    elif neighbour.pronouns is not None and neighbour.pronouns is not None:\n",
    "                        relates_to_prev = True\n",
    "            else:\n",
    "                if type == \"OBJ\" and index > 0:\n",
    "                    for obj in self.obj[0:index]:\n",
    "                        if obj.get_phrase_classification() is not None:\n",
    "                            resolution.relates_to = obj\n",
    "                            resolution.classification = obj.get_phrase_classification()\n",
    "                            return (resolution, parsed_subj)\n",
    "                elif type == \"SUBJ\" and index > 0:\n",
    "                    for subj in self.subj[0:index]:\n",
    "                        if subj.get_phrase_classification() is not None:\n",
    "                            resolution.relates_to = subj\n",
    "                            resolution.classification = subj.get_phrase_classification()\n",
    "                            return (resolution, parsed_subj)\n",
    "\n",
    "                # Bezieht sich auf Subjekt\n",
    "                for subj in self.subj:\n",
    "                    if subj.get_phrase_classification() is not None:\n",
    "                        resolution.relates_to = subj\n",
    "                        resolution.classification = subj.get_phrase_classification()\n",
    "                        return (resolution, parsed_subj)\n",
    "                    elif subj.pronouns is not None and subj.pronouns is not None:\n",
    "                        relates_to_prev = True\n",
    "                                \n",
    "\n",
    "        if (pronom['type'] != \"poss\") or (relates_to_prev == True):\n",
    "            parsed_subj == True\n",
    "            if self.parse_prev_clauses(prev_clauses) is not None:\n",
    "                relates_to, classification = self.parse_prev_clauses(prev_clauses)\n",
    "                resolution.relates_to = relates_to\n",
    "                resolution.classification = classification\n",
    "\n",
    "        return (resolution, parsed_subj)\n",
    "\n",
    "    def parse_prev_clauses(self, prev_clauses):\n",
    "        for prev_c in prev_clauses:\n",
    "            for subj in prev_c.subj:\n",
    "                if subj.get_phrase_classification() is not None:\n",
    "                    return(subj, subj.get_phrase_classification())\n",
    "                else:\n",
    "                    for sub in subj.sub_phrases:\n",
    "                        if sub.get_phrase_classification() is not None:\n",
    "                            return(sub, sub.get_phrase_classification())\n",
    "                            \n",
    "            for obj in prev_c.obj:\n",
    "                if obj.get_phrase_classification() is not None:\n",
    "                    return(obj, obj.get_phrase_classification())\n",
    "                \n",
    "                for sub in obj.sub_phrases:\n",
    "                    if sub.get_phrase_classification() is not None:\n",
    "                        return(sub, sub.get_phrase_classification())\n",
    "\n",
    "\n",
    "class CompoundSentence:\n",
    "    def __init__(self, clauses = []):\n",
    "        self.clauses = clauses\n",
    "\n",
    "    def do_parsing(self):\n",
    "        for clause in self.clauses:\n",
    "            clause.do_parsing()\n",
    "        # DO Parsing\n",
    "        self.clauses = sorted(self.clauses, key=lambda x: x.index)\n",
    "\n",
    "    def pronom_relation(self):\n",
    "        resolutions = []\n",
    "        prev_clauses = []\n",
    "        for clause in self.clauses:\n",
    "            resolutions.extend(clause.pronom_relation(prev_clauses))\n",
    "            prev_clauses.append(clause)\n",
    "        return resolutions\n",
    "\n",
    "\n",
    "class PronomResolution:\n",
    "    def __init__(self, pronom):\n",
    "        self.pronom = pronom\n",
    "        self.relates_to = None\n",
    "        self.classification = None\n",
    "\n",
    "    def get_person(self):\n",
    "        return self.pronom['person']\n",
    "    \n",
    "    def get_numerus(self):\n",
    "        return \"plural\" if self.pronom['plural'] else \"singular\"\n",
    "    \n",
    "    def get_classification(self):\n",
    "        return self.classification\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.pronom['person']) + \"-\" + (\"plural\" if self.pronom['plural'] else \"singular\")+ \"-\" + self.pronom['type'] + \"-\" + str(self.classification) + \" (\"+ self.pronom['token'].text +\")\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_children(root, depth=0):\n",
    "    tokens = []\n",
    "    phrases = []\n",
    "    clauses = []\n",
    "    phrase = None\n",
    "    clause = None\n",
    "\n",
    "    # Create Clause if no clause is found\n",
    "    if depth == 0:\n",
    "        clause = Clause(type = \"main\", phrases=[])\n",
    "        clauses.append(clause)\n",
    "\n",
    "    # Create Clause based on found clause\n",
    "    if \"cl\" in root.dep_ and root.dep_ != \"acl\":\n",
    "        clause = Clause(type = root.dep_, phrases=[])\n",
    "        clauses.append(clause)\n",
    "    \n",
    "    # Iterate over children\n",
    "    for child in root.children:\n",
    "\n",
    "        # Check if new Subject or Object Phrase is detected\n",
    "        if \"subj\" in child.dep_ or \"obj\" in child.dep_:\n",
    "            # create type subj or obj\n",
    "            type = \"subj\" if \"subj\" in child.dep_ else \"obj\"\n",
    "            dep = child.dep_\n",
    "            # create Phrase containing all nestedtokens and phrases\n",
    "            phrase = Phrase(type, dep, [], [])\n",
    "            tokens = []\n",
    "            # Append phrase to Phrase List\n",
    "            phrases.append(phrase)\n",
    "\n",
    "        # Append Token\n",
    "        if(phrase != None):\n",
    "            phrase.tokens.append(child)\n",
    "        else:\n",
    "            tokens.append(child)\n",
    "\n",
    "        # process childs childrens data\n",
    "        if len(list(child.children)) > 0:\n",
    "            # Parse Child\n",
    "            to, ph, cl = parse_children(child, depth+1)\n",
    "            # Append Parsed childs data\n",
    "            if phrase != None:\n",
    "                phrase.tokens.extend(to)\n",
    "                phrase.sub_phrases.extend(ph)\n",
    "            else:\n",
    "                tokens.extend(to)\n",
    "                phrases.extend(ph)\n",
    "\n",
    "            clauses.extend(cl)\n",
    "\n",
    "        if \"subj\" in child.dep_ or \"obj\" in child.dep_:\n",
    "            tokens = []\n",
    "            phrase = None\n",
    "\n",
    "    if clause != None:\n",
    "        clause.append_phrases(phrases)\n",
    "        phrases=[]\n",
    "        \n",
    "\n",
    "    return (tokens, phrases, clauses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text):\n",
    "    doc = nlp(text) \n",
    "    sentences = []\n",
    "    classifications = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"ROOT\":\n",
    "            #print(token)\n",
    "            to, ph, cl = parse_children(token)\n",
    "            sentence = CompoundSentence(clauses=cl)\n",
    "\n",
    "            sentence.do_parsing()\n",
    "            classifications.extend(sentence.pronom_relation())\n",
    "            \n",
    "            sentences.append(sentence)\n",
    "    return classifications\n",
    "\n",
    "def format_classifications(classifications):\n",
    "    fmt_class = []\n",
    "    for classification in classifications:\n",
    "        if classification.get_person() == \"3\" and classification.get_numerus() == \"plural\" and classification.get_classification() == \"GENERIC\":\n",
    "            fmt_class.append(\"GENERIC-THEY\")\n",
    "        elif classification.get_person() == \"3\" and classification.get_numerus() == \"plural\" and classification.get_classification() == \"SINGULAR\":\n",
    "            fmt_class.append(\"NON-BINARY-THEY\")\n",
    "        elif classification.get_person() == \"3\" and classification.get_numerus() == \"singular\" and classification.get_classification() == \"GENERIC\":\n",
    "            fmt_class.append(\"GENERIC-HE\")\n",
    "    return str(list(set(fmt_class)))\n",
    "\n",
    "def classification_to_str(classification):\n",
    "    str_class = []\n",
    "    for classi in classification:\n",
    "        str_class.append(str(classi))\n",
    "    return str_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_text_path = \"/text/\"\n",
    "classified_text_path = \"/classified/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get List of Months\n",
    "total_docs = 0\n",
    "months = []\n",
    "for txtfile in os.listdir(corpus_text_path):\n",
    "    month = re.search('.*(\\d\\d[-_]\\d\\d)-.*', txtfile, re.IGNORECASE)\n",
    "    month_alt = re.search('.*(\\d\\d[-_]\\d\\d)[-_]\\d\\d-.*', txtfile, re.IGNORECASE)\n",
    "    if month is not None:\n",
    "        month = month.group(1)\n",
    "        if month_alt is not None:\n",
    "            month = month_alt.group(1)\n",
    "        months.append(month)\n",
    "    else:\n",
    "        print(txtfile)\n",
    "\n",
    "total_docs = len(months)\n",
    "months = list(set(months))\n",
    "months.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_month(month):\n",
    "    print(\"started processing: \", month)\n",
    "\n",
    "    month_df = None\n",
    "\n",
    "    for txtfile in os.listdir(corpus_text_path):\n",
    "        txtmonth = re.search('.*(\\d\\d[-_]\\d\\d)-.*', txtfile, re.IGNORECASE)\n",
    "        month_alt = re.search('.*(\\d\\d[-_]\\d\\d)[-_]\\d\\d-.*', txtfile, re.IGNORECASE)\n",
    "        if txtmonth is not None:\n",
    "            txtmonth = txtmonth.group(1)\n",
    "            if month_alt is not None:\n",
    "                txtmonth = month_alt.group(1)\n",
    "                \n",
    "            if txtmonth == month:\n",
    "                texts = pd.read_csv(os.path.join(corpus_text_path,txtfile), delimiter=\"\\t\", encoding = \"ISO-8859-1\", usecols=[0], names=['text'], header=None)\n",
    "                texts['id'] = texts['text'].str.extract(r'\\@\\@(\\d*)')\n",
    "                texts['text'] = texts['text'].str.extract(r'\\@\\@\\d* (.*)')\n",
    "                texts['text'] = texts['text'].str.replace(r'\\<.*\\>', \"\", regex=True)\n",
    "                texts = texts[['text', 'id']]\n",
    "\n",
    "                if len(texts.loc[(texts['id'].isna()) | (texts['id'].isnull())]) > 1:\n",
    "                    print(\"Fehler: \", txtfile, \" | Na/Null:\", len(texts.loc[(texts['id'].isna()) | (texts['id'].isnull())]))\n",
    "                \n",
    "                texts = texts.loc[(texts['id'].notna()) & (texts['id'].notnull())]\n",
    "\n",
    "                if len(texts) < 10:\n",
    "                    print(\"Warnung: \", txtfile, \" | Zeilen:\", len(texts))\n",
    "\n",
    "\n",
    "                if month_df is None:\n",
    "                    month_df = texts\n",
    "                else:\n",
    "                    month_df = pd.concat([month_df, texts], ignore_index=True)\n",
    "    \n",
    "    month_df = month_df.loc[month_df['text'].isnull() == False]\n",
    "\n",
    "    month_df['classification'] = month_df['text'].swifter.apply(lambda x:classify_text(x))\n",
    "    month_df['labels'] = month_df['classification'].swifter.apply(lambda x:format_classifications(x))\n",
    "\n",
    "    month_df['GENERIC-THEY'] = month_df['labels'].swifter.apply(lambda x: \"GENERIC-THEY\" in x)\n",
    "    month_df['GENERIC-HE'] = month_df['labels'].swifter.apply(lambda x: \"GENERIC-HE\" in x)\n",
    "    month_df['NON-BINARY-THEY'] = month_df['labels'].swifter.apply(lambda x: \"NON-BINARY-THEY\" in x)\n",
    "\n",
    "    month_df['classification'] = month_df['classification'].swifter.apply(lambda x:classification_to_str(x))\n",
    "    month_df.to_feather(os.path.join(classified_text_path, month+\".feather\"))\n",
    "    \n",
    "    print(\"finished processing: \", month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for month in months:\n",
    "    process_month(month)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
